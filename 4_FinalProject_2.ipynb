{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#This file shows Word2Vec, Doc2Vec, Glove, and various models implemented on them after oversampling is done on the dataframe"
      ],
      "metadata": {
        "id": "Ol_K0lG4kEge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Importing libraries and preprocessing ###"
      ],
      "metadata": {
        "id": "nlioHoFmeVP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42) # results are reproducible and comparable across different runs"
      ],
      "metadata": {
        "id": "WUmERbCIDih7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "CMD = ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "CR = classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import gensim\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re"
      ],
      "metadata": {
        "id": "NhGzXxQHOK8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "RYOzsCJj_Cdq",
        "outputId": "7a31d44c-af3a-48bc-f4f4-1dacac222150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5350, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  fake\n",
              "0  Modern two-dimensional imaging is of such qual...     0\n",
              "1  Background: The optimal sequence of systemic p...     1\n",
              "2  This chapter opens with a discussion of the ef...     1\n",
              "3  The time scale of the ultra-short-term can str...     1\n",
              "4  Electronic nose or machine olfaction are syste...     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9da7f51c-fc19-4372-a1b8-085415fbf916\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Modern two-dimensional imaging is of such qual...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Background: The optimal sequence of systemic p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This chapter opens with a discussion of the ef...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The time scale of the ultra-short-term can str...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Electronic nose or machine olfaction are syste...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9da7f51c-fc19-4372-a1b8-085415fbf916')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9da7f51c-fc19-4372-a1b8-085415fbf916 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9da7f51c-fc19-4372-a1b8-085415fbf916');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df = pd.read_csv('/content/fake_papers_train_part_public.csv')\n",
        "df = df.drop(columns = ['id'], axis = 1)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n",
        "    doc = nlp(text)\n",
        "    text = [token.lemma_ for token in doc]\n",
        "    return ' '.join(text)\n",
        "\n",
        "# Apply the clean_text function to each row of the 'text' column in the dataframe\n",
        "df['cleanedtext'] = df['text'].apply(lambda x: clean_text(x.lower()))\n",
        "\n",
        "# Create a new column to store the lists of cleaned text for each document\n",
        "df['cleanedtext_list'] = None\n",
        "\n",
        "# Loop through each row in the dataframe and store the cleaned text for each document in a list\n",
        "for index, row in df.iterrows():\n",
        "    cleaned_text_list = row['cleanedtext'].split()\n",
        "    df.at[index, 'cleanedtext_list'] = cleaned_text_list\n",
        "df = df.drop('cleanedtext', axis=1)\n",
        "\n",
        "# Print the first 5 rows of the dataframe to check the new column\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8-G0L9oW4rBY",
        "outputId": "63e89747-bfd7-4f49-af48-429b457d207f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  fake  \\\n",
              "0  Modern two-dimensional imaging is of such qual...     0   \n",
              "1  Background: The optimal sequence of systemic p...     1   \n",
              "2  This chapter opens with a discussion of the ef...     1   \n",
              "3  The time scale of the ultra-short-term can str...     1   \n",
              "4  Electronic nose or machine olfaction are syste...     1   \n",
              "\n",
              "                                    cleanedtext_list  \n",
              "0  [modern, twodimensional, imaging, be, of, such...  \n",
              "1  [background, the, optimal, sequence, of, syste...  \n",
              "2  [this, chapter, open, with, a, discussion, of,...  \n",
              "3  [the, time, scale, of, the, ultrashortterm, ca...  \n",
              "4  [electronic, nose, or, machine, olfaction, be,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34ea6f7e-2375-4b47-befe-adfb77557693\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>fake</th>\n",
              "      <th>cleanedtext_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Modern two-dimensional imaging is of such qual...</td>\n",
              "      <td>0</td>\n",
              "      <td>[modern, twodimensional, imaging, be, of, such...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Background: The optimal sequence of systemic p...</td>\n",
              "      <td>1</td>\n",
              "      <td>[background, the, optimal, sequence, of, syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This chapter opens with a discussion of the ef...</td>\n",
              "      <td>1</td>\n",
              "      <td>[this, chapter, open, with, a, discussion, of,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The time scale of the ultra-short-term can str...</td>\n",
              "      <td>1</td>\n",
              "      <td>[the, time, scale, of, the, ultrashortterm, ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Electronic nose or machine olfaction are syste...</td>\n",
              "      <td>1</td>\n",
              "      <td>[electronic, nose, or, machine, olfaction, be,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34ea6f7e-2375-4b47-befe-adfb77557693')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34ea6f7e-2375-4b47-befe-adfb77557693 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34ea6f7e-2375-4b47-befe-adfb77557693');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['fake'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HONnM1V_RGTk",
        "outputId": "65ef6189-b92d-4c77-c9ea-ea744fa317ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    3664\n",
              "0    1686\n",
              "Name: fake, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmxPORq0c5oZ"
      },
      "outputs": [],
      "source": [
        "X = list(df['cleanedtext_list'])\n",
        "y = list(df['fake'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_val_train, X_test, y_val_train, y_test = train_test_split(df['cleanedtext_list'], df['fake'], test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "-6FfS9UQDUMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBwkTgHxBJCT",
        "outputId": "659a4787-7042-4e10-9040-28bd36e2ec65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word2Vec and Doc2Vec"
      ],
      "metadata": {
        "id": "lOLlHFDSjwLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest on Word2Vec"
      ],
      "metadata": {
        "id": "t6JRiYzYjpw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train a Skip Gram Word2Vec model on the training data\n",
        "w2v_model = gensim.models.Word2Vec(X_val_train,\n",
        "                                   vector_size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2,\n",
        "                                   workers=4,\n",
        "                                   sg=1)  # set sg=1 to use the Skip Gram algorithm\n",
        "\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Vectorize the training and test data using the trained Skip Gram Word2Vec model\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                        for ls in X_test])\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
        "# did not learn any of the words in the text message during training)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "rf_model = rf.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xikL5X3HQIR",
        "outputId": "dfa835b4-6e06-4402-9918-a61b1db807d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-9986bacf82f4>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "<ipython-input-49-9986bacf82f4>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.986 / Recall: 0.974 / Accuracy: 0.973\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[324  10]\n",
            " [ 19 717]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest on Doc2Vec"
      ],
      "metadata": {
        "id": "-QyivhnXjdXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the training data to TaggedDocuments for Doc2Vec\n",
        "tagged_docs_train = [TaggedDocument(words=words, tags=[idx]) for idx, words in enumerate(X_val_train)]\n",
        "\n",
        "# Train a Doc2Vec model on the training data\n",
        "d2v_model = Doc2Vec(tagged_docs_train,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)\n",
        "\n",
        "# Vectorize the training and test data using the trained Doc2Vec model\n",
        "X_train_vect = np.array([d2v_model.infer_vector(words) for words in X_val_train])\n",
        "X_test_vect = np.array([d2v_model.infer_vector(words) for words in X_test])\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect, y_train_np)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "rf_model = rf.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyXyVcEJLP7H",
        "outputId": "391b25c7-dd32-4076-820d-3a0a5650aa73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.91 / Recall: 0.938 / Accuracy: 0.893\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[266  68]\n",
            " [ 46 690]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM on Word2Vec"
      ],
      "metadata": {
        "id": "L0SDqK70jZK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Train a Skip Gram Word2Vec model on the training data\n",
        "w2v_model = gensim.models.Word2Vec(X_val_train,\n",
        "                                   vector_size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2,\n",
        "                                   workers=4,\n",
        "                                   sg=1)  # set sg=1 to use the Skip Gram algorithm\n",
        "\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Vectorize the training and test data using the trained Skip Gram Word2Vec model\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                        for ls in X_test])\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
        "# did not learn any of the words in the text message during training)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "svm_model = svm.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = svm_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cItq1soLH-_L",
        "outputId": "770532c8-47f1-4ed1-b19d-227b9bbebfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-c00b2bd39275>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "<ipython-input-50-c00b2bd39275>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.937 / Recall: 0.772 / Accuracy: 0.807\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[296  38]\n",
            " [168 568]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM on Doc2Vec"
      ],
      "metadata": {
        "id": "SMwrAOCFjB9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Train a Doc2Vec model on the training data\n",
        "tagged_documents = [TaggedDocument(words=words, tags=[idx]) for idx, words in enumerate(X_val_train)]\n",
        "d2v_model = Doc2Vec(tagged_documents,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)\n",
        "\n",
        "# Vectorize the training and test data using the trained Doc2Vec model\n",
        "X_train_vect = np.array([d2v_model.infer_vector(words) for words in X_val_train])\n",
        "X_test_vect = np.array([d2v_model.infer_vector(words) for words in X_test])\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect = np.array(X_train_vect)\n",
        "X_test_vect = np.array(X_test_vect)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect, y_train_np)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "svm_model = svm.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = svm_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNMs4eNoI90I",
        "outputId": "108ba641-aede-4484-d872-4d93eed959fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.925 / Recall: 0.857 / Accuracy: 0.854\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[283  51]\n",
            " [105 631]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree on Word2Vec"
      ],
      "metadata": {
        "id": "rNC2dCfzi8qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Train a Word2Vec model on the training data\n",
        "w2v_model = gensim.models.Word2Vec(X_val_train,\n",
        "                                   vector_size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2,\n",
        "                                   workers=4)\n",
        "\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Vectorize the training and test data using the trained Word2Vec model\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                        for ls in X_test])\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
        "# did not learn any of the words in the text message during training)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "dt_model = dt.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = dt_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxeM3z17J9Dp",
        "outputId": "ffa99cc0-103e-48c0-ed62-c4dbb3c017ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-ae3a08045676>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "<ipython-input-53-ae3a08045676>:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.969 / Recall: 0.966 / Accuracy: 0.955\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[311  23]\n",
            " [ 25 711]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desicion Tree on Doc2Vec"
      ],
      "metadata": {
        "id": "Ovqx6cS7i4rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Train a Doc2Vec model on the training data\n",
        "tagged_documents = [TaggedDocument(words=words, tags=[idx]) for idx, words in enumerate(X_val_train)]\n",
        "d2v_model = Doc2Vec(tagged_documents,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)\n",
        "\n",
        "# Vectorize the training and test data using the trained Doc2Vec model\n",
        "X_train_vect = np.array([d2v_model.infer_vector(words) for words in X_val_train])\n",
        "X_test_vect = np.array([d2v_model.infer_vector(words) for words in X_test])\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect = np.array(X_train_vect)\n",
        "X_test_vect = np.array(X_test_vect)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect, y_train_np)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "dt_model = dt.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = dt_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyyDixVRJFEv",
        "outputId": "543810da-29ae-40b4-daef-f990d31aefb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.844 / Recall: 0.787 / Accuracy: 0.753\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[227 107]\n",
            " [157 579]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Classifier on Word2Vec"
      ],
      "metadata": {
        "id": "lb5XjM4FizEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Train a Word2Vec model on the training data\n",
        "w2v_model = gensim.models.Word2Vec(X_val_train,\n",
        "                                   vector_size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2,\n",
        "                                   workers=4)\n",
        "\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Vectorize the training and test data using the trained Word2Vec model\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                        for ls in X_test])\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
        "# did not learn any of the words in the text message during training)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a 1-layer MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='adam')\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "mlp_model = mlp.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = mlp_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH85nYrAJiEA",
        "outputId": "383e1de9-58d5-4898-ac1b-744a15a05872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-7fe373e4ba54>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "<ipython-input-54-7fe373e4ba54>:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.907 / Recall: 0.849 / Accuracy: 0.836\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[270  64]\n",
            " [111 625]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLPClassifier on Doc2Vec"
      ],
      "metadata": {
        "id": "c4NuhIVsipXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "\n",
        "# Convert the training data to TaggedDocuments for Doc2Vec\n",
        "tagged_docs_train = [TaggedDocument(words=words, tags=[idx]) for idx, words in enumerate(X_val_train)]\n",
        "\n",
        "# Train a Doc2Vec model on the training data\n",
        "d2v_model = Doc2Vec(tagged_docs_train,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)\n",
        "\n",
        "# Vectorize the training and test data using the trained Doc2Vec model\n",
        "X_train_vect = np.array([d2v_model.infer_vector(words) for words in X_val_train])\n",
        "X_test_vect = np.array([d2v_model.infer_vector(words) for words in X_test])\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect, y_train_np)\n",
        "\n",
        "# Create a 1-layer MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='adam')\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "mlp_model = mlp.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = mlp_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l61rBdkKLfd",
        "outputId": "64313f09-e080-450c-f2da-49f69dfc8b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.934 / Recall: 0.943 / Accuracy: 0.915\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[285  49]\n",
            " [ 42 694]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec using Logistic Regression"
      ],
      "metadata": {
        "id": "EZIUb-gYiUOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train a Skip Gram Word2Vec model on the training data\n",
        "w2v_model = gensim.models.Word2Vec(X_val_train,\n",
        "                                   vector_size=100,\n",
        "                                   window=5,\n",
        "                                   min_count=2,\n",
        "                                   workers=4,\n",
        "                                   sg=1)  # set sg=1 to use the Skip Gram algorithm\n",
        "\n",
        "words = set(w2v_model.wv.index_to_key)\n",
        "\n",
        "# Vectorize the training and test data using the trained Skip Gram Word2Vec model\n",
        "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
        "                        for ls in X_test])\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if the model\n",
        "# did not learn any of the words in the text message during training)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Logistic Regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "lr_model = lr.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = lr_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoWJWWrfOgAc",
        "outputId": "2e24aecb-1acc-49f8-f2de-0b1b8f501a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-2dbeff1e3b4e>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
            "<ipython-input-61-2dbeff1e3b4e>:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.884 / Recall: 0.736 / Accuracy: 0.752\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[263  71]\n",
            " [194 542]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression on Doc2Vec"
      ],
      "metadata": {
        "id": "3ywUO7oyg6JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the training data to TaggedDocuments for Doc2Vec\n",
        "tagged_docs_train = [TaggedDocument(words=words, tags=[idx]) for idx, words in enumerate(X_val_train)]\n",
        "\n",
        "# Train a Doc2Vec model on the training data\n",
        "d2v_model = Doc2Vec(tagged_docs_train,\n",
        "                    vector_size=100,\n",
        "                    window=5,\n",
        "                    min_count=2,\n",
        "                    workers=4)\n",
        "\n",
        "# Vectorize the training and test data using the trained Doc2Vec model\n",
        "X_train_vect = np.array([d2v_model.infer_vector(words) for words in X_val_train])\n",
        "X_test_vect = np.array([d2v_model.infer_vector(words) for words in X_test])\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect, y_train_np)\n",
        "\n",
        "# Create a Logistic Regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "lr_model = lr.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = lr_model.predict(X_test_vect)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgO7ERscOghU",
        "outputId": "03d00052-6b1f-444a-b46b-4dc55be1f0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.87 / Recall: 0.766 / Accuracy: 0.761\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[250  84]\n",
            " [172 564]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Glove"
      ],
      "metadata": {
        "id": "UEcgDqn2SYgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained GloVe model\n",
        "model = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB6ZbLmCTR3z",
        "outputId": "7fe9e6e1-8611-4c55-e8df-6ac843148843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d glove\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4-7BMuHTP6B",
        "outputId": "1a80dab9-a54a-4de6-b28f-326128b9ddd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-15 16:53:24--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-05-15 16:53:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-05-15 16:53:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.81MB/s    in 2m 52s  \n",
            "\n",
            "2023-05-15 16:56:17 (4.78 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_index = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = '/content/glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_path)"
      ],
      "metadata": {
        "id": "_C0Pec-NSbFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the training and test data using GloVe embeddings\n",
        "X_train_vect = np.array([np.array([glove_embeddings.get(i, np.zeros(100)) for i in ls if i in words])\n",
        "                         for ls in X_val_train])\n",
        "X_test_vect = np.array([np.array([glove_embeddings.get(i, np.zeros(100)) for i in ls if i in words])\n",
        "                        for ls in X_test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJneG7APT1Vx",
        "outputId": "397e8324-4fce-442a-9c4b-9f649be5171a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-1083dbf9d124>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_train_vect = np.array([np.array([glove_embeddings.get(i, np.zeros(100)) for i in ls if i in words])\n",
            "<ipython-input-67-1083dbf9d124>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X_test_vect = np.array([np.array([glove_embeddings.get(i, np.zeros(100)) for i in ls if i in words])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Fz4X5gAMWtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove using RandomForest"
      ],
      "metadata": {
        "id": "-jfrrQbhhPla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if no embedding is found)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "rf_model = rf.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = rf_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y08O8DPUxT8",
        "outputId": "eb594507-eaa5-493a-9270-a3dbe1c484e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.873 / Recall: 0.978 / Accuracy: 0.887\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[229 105]\n",
            " [ 16 720]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove using Logistic Regression"
      ],
      "metadata": {
        "id": "JMcv1pMfhjKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if no embedding is found)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Logistic Regression classifier\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "lr_model = lr.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = lr_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1R4mOcQWWRv",
        "outputId": "eb59e02f-e2ab-4745-96c8-556fc863b800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.752 / Recall: 0.769 / Accuracy: 0.666\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[147 187]\n",
            " [170 566]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desicion Tree using Glove"
      ],
      "metadata": {
        "id": "MD41FDAKhpqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if no embedding is found)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "dt_model = dt.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = dt_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8-MbCExWvAg",
        "outputId": "3d8198a8-3e24-4957-e34a-63afa9ee7f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.87 / Recall: 0.97 / Accuracy: 0.879\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[227 107]\n",
            " [ 22 714]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove using SVC"
      ],
      "metadata": {
        "id": "mKk_RBXYiAwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if no embedding is found)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "svm_model = svm.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = svm_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2qsaiK5XQ2f",
        "outputId": "64f5a977-68b5-43c7-c31a-ec90071f1e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.823 / Recall: 0.829 / Accuracy: 0.76\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[203 131]\n",
            " [126 610]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove using MLP Classifier"
      ],
      "metadata": {
        "id": "KrTJQJI9iDpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Average the word vectors for each sentence (and assign a vector of zeros if no embedding is found)\n",
        "X_train_vect_avg = []\n",
        "for v in X_train_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "X_test_vect_avg = []\n",
        "for v in X_test_vect:\n",
        "    if v.size:\n",
        "        X_test_vect_avg.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
        "\n",
        "# Convert the data to numpy arrays\n",
        "X_train_vect_avg = np.array(X_train_vect_avg)\n",
        "X_test_vect_avg = np.array(X_test_vect_avg)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train_np = y_val_train.values.ravel()\n",
        "y_test_np = pd.DataFrame(y_test).values.ravel()\n",
        "\n",
        "# Perform oversampling using SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_vect_avg, y_train_np)\n",
        "\n",
        "# Create an MLP classifier with 1 hidden layer\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
        "\n",
        "# Fit the model to the oversampled training data\n",
        "mlp_model = mlp.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "y_pred = mlp_model.predict(X_test_vect_avg)\n",
        "\n",
        "# Evaluate the predictions of the model on the holdout test set\n",
        "precision = precision_score(y_test_np, y_pred)\n",
        "recall = recall_score(y_test_np, y_pred)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_np, y_pred)\n",
        "\n",
        "print('\\n\\nPrecision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
        "print('\\n\\nConfusion Matrix:')\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibV_95X0XiPG",
        "outputId": "cecd1d5e-974d-4f76-e872-e8ee17cd4466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Precision: 0.847 / Recall: 0.859 / Accuracy: 0.796\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[220 114]\n",
            " [104 632]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IbgH1bXSLuTV",
        "hEMKX8yuA9uy",
        "UEcgDqn2SYgK"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}